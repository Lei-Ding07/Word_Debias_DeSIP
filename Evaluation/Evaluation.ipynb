{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe0414b-0f8a-4f9c-82ef-c2a222c47f61",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "* Gonen, H.; and Goldberg, Y. 2019. Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them. NAACL-HLT.\n",
    "* Yang, Z.; and Feng, J. 2020. A causal inference method for reducing gender bias in word embedding relations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, 9434–9441."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a3eef2-b9fe-4589-ad86-d431010e2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy, requests, codecs, os, re, nltk, itertools, csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from scipy.stats import spearmanr\n",
    "import functools as ft\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659378d0-246a-463b-a077-d66b7abb5065",
   "metadata": {},
   "source": [
    "## Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546c31c5-e314-4c74-8521-13b2dcb10e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debias word vector file name\n",
    "file_name = \"P_DeSIP_vectors.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a5776a-928d-4e4a-80dd-3faa46bb1a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322636\n"
     ]
    }
   ],
   "source": [
    "VEC_LEN = 300\n",
    "glove_file = open(\"./Data/WordEmbedding/glove_wiki_vectors.txt\", 'r')\n",
    "glove_word = {}\n",
    "for line in glove_file:\n",
    "    line = line.strip()\n",
    "    _word = line.split(' ')\n",
    "    vector = np.array([float(num) for num in _word[1:]])\n",
    "    if len(vector) != VEC_LEN: \n",
    "        raise Exception(\"Word dimension is wrong\")\n",
    "    glove_word[_word[0]] = vector\n",
    "glove_file.close()\n",
    "print(len(glove_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2656c09f-5d8d-4897-8a19-d16f6ff3e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VEC_LEN = 300\n",
    "debias_file = open(\"./Data/WordEmbedding/\" + file_name, 'r')\n",
    "debias_word = {}\n",
    "for line in debias_file:\n",
    "    line = line.strip()\n",
    "    _word = line.split(' ')\n",
    "    vector = np.array([float(num) for num in _word[1:]])\n",
    "    if len(vector) != VEC_LEN: \n",
    "        raise Exception(\"Word dimension is wrong\")\n",
    "    debias_word[_word[0]] = vector\n",
    "debias_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1152f7ba-b36b-4d8c-b270-ae370335e40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322636"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(debias_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6846a16-2a76-4567-8ab0-dabc9461d8d4",
   "metadata": {},
   "source": [
    "## SemBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4159654-b600-433a-b069-1d873a577c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_SemBias(filename = './Data/SemBias.txt'):\n",
    "    '''\n",
    "    Read the words in SemBias text\n",
    "    Args: \n",
    "        filename - str, the name of SemBias document\n",
    "    Returns:\n",
    "        SemBias_list - list[list[list[str]]], the element is a word analogy test, including four pairs of words\n",
    "                        e.g. [['priest', 'nun'], ['cup', 'lid'], ['algebra', 'geometry'], ['manager', 'secretary']]\n",
    "    '''\n",
    "    file_read = open(filename, \"r\", encoding = 'utf8')\n",
    "    \n",
    "    SemBias_list = []\n",
    "    \n",
    "    for line in file_read:\n",
    "        pairs = line.rstrip().split('\\t')\n",
    "        \n",
    "        line_temp = []\n",
    "        for p in pairs:\n",
    "            a, b = p.split(':')\n",
    "            line_temp.append([a,b])\n",
    "        \n",
    "        SemBias_list.append(line_temp)\n",
    "        \n",
    "    file_read.close()\n",
    "    \n",
    "    return SemBias_list\n",
    "\n",
    "SemBias_task = read_SemBias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516ea5e2-f6e2-42f4-9381-28f84142758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_top(task, wordVecModel):\n",
    "    '''\n",
    "    Evaluate the accuracy of Sembias tests\n",
    "    Args:\n",
    "        task - list[list[list[str]]], the set of Sembias tests\n",
    "        wordVecModel - dict{str:np.array}, the word vector dictionary\n",
    "    Returns:\n",
    "        result_list - list[int], the list of results of Sembias tests where the result is index of best analogy\n",
    "    '''\n",
    "    result_list = []\n",
    "    he_we = wordVecModel['he'].reshape(1,-1)\n",
    "    she_we = wordVecModel['she'].reshape(1,-1)\n",
    "    \n",
    "    for line in task:\n",
    "        temp_score = []\n",
    "        \n",
    "        if len(line) != 4:\n",
    "            print('error')\n",
    "            \n",
    "        for pair in line:\n",
    "            (word_i, word_j) = pair\n",
    "            current_distance = cosine_similarity(he_we - she_we , wordVecModel[word_i].reshape(1,-1) - wordVecModel[word_j].reshape(1,-1) )        \n",
    "            temp_score.append(current_distance)\n",
    "        \n",
    "        result_list.append(temp_score.index(max(temp_score)))\n",
    "        \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b56969c1-9fff-454a-a7e4-2a86fa9485e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemBias task : \n",
      "440 instances, each instance: (gender, bias, normal, normal) 4 pairs\n",
      "Sembias test result of the debiased word embedding\n",
      " 0.9522727272727273\n",
      "Sembias subset test result of the debiased word embedding\n",
      " 0.975\n"
     ]
    }
   ],
   "source": [
    "print(\"SemBias task : \\n440 instances, each instance: (gender, bias, normal, normal) 4 pairs\")\n",
    "xx_debias = eval_top(SemBias_task, debias_word)\n",
    "print(\"Sembias test result of the debiased word embedding\\n\", xx_debias.count(0)/440)\n",
    "print(\"Sembias subset test result of the debiased word embedding\\n\", xx_debias[-40:].count(0)/40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92f19d-0812-47bf-8a26-b6ec4d906efe",
   "metadata": {},
   "source": [
    "## Word Similarity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a81b083-2b7e-4364-a87a-8e9825fc2bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Read the male words and female words\n",
    "'''\n",
    "gender_specific = []\n",
    "with open('./Data/male_word_file.txt') as f:\n",
    "    for l in f:\n",
    "        gender_specific.append(l.strip())\n",
    "with open('./Data/female_word_file.txt') as f:\n",
    "    for l in f:\n",
    "        gender_specific.append(l.strip())\n",
    "\n",
    "exclude_words = gender_specific\n",
    "print(len(exclude_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e95c1563-c4c8-4258-96da-efee09e78a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 730379.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 47698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Generate word list for evaluation\n",
    "'''\n",
    "import string \n",
    "from tqdm import tqdm\n",
    "\n",
    "def has_punct(w):\n",
    "    '''\n",
    "    Examine if the word includes punctuations\n",
    "    Arg:\n",
    "        w - str, word for examining\n",
    "    Returns:\n",
    "        bool\n",
    "    '''\n",
    "    if any([c in string.punctuation for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def has_digit(w):\n",
    "    '''\n",
    "    Examine if the word includes numbers\n",
    "    Arg:\n",
    "        w - str, word for examining\n",
    "    Returns:\n",
    "        bool\n",
    "    '''\n",
    "    if any([c in '0123456789' for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def limit_vocab(word_embedding, exclude = None, vec_len = 300):\n",
    "    '''\n",
    "    Generate the word list for evaluation\n",
    "    Arg:\n",
    "        word_embedding - dict{str:np.array}, the word vector dictionary\n",
    "        exclude - list[str], the list of gender words\n",
    "        vec_len - int, the number of dimentions of word vector\n",
    "    Returns:\n",
    "        vocab_limited - list[str], the evaluation word list\n",
    "    '''\n",
    "    vocab_limited = []\n",
    "    for w in tqdm(list(word_embedding.keys())[:50000]): \n",
    "        if w.lower() != w:\n",
    "            continue\n",
    "        if len(w) >= 20:\n",
    "            continue\n",
    "        if has_digit(w):\n",
    "            continue\n",
    "        if '_' in w:\n",
    "            p = [has_punct(subw) for subw in w.split('_')]\n",
    "            if not any(p):\n",
    "                vocab_limited.append(w)\n",
    "            continue\n",
    "        if has_punct(w):\n",
    "            continue\n",
    "        vocab_limited.append(w)\n",
    "    \n",
    "    if exclude:\n",
    "        vocab_limited = list(set(vocab_limited) - set(exclude))\n",
    "    \n",
    "    print(\"size of vocabulary:\", len(vocab_limited))\n",
    "    \n",
    "    return vocab_limited\n",
    "\n",
    "word_limited = limit_vocab(glove_word, exclude_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0248a925-2d66-4f5c-b35b-2ad004a02e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate Pearson correlation coeficient\n",
    "'''\n",
    "import scipy.stats\n",
    "\n",
    "def pearson(a,b):\n",
    "   \n",
    "    return scipy.stats.pearsonr(a,b)\n",
    "\n",
    "def compute_corr(tuples, i1, i2):\n",
    "    '''\n",
    "    Calculate Pearson correlation coeficient between two lists\n",
    "    Args:\n",
    "        tuples - list[list[int]], the elements are result of each test\n",
    "        i1 - int, for each test, the index of number to form first list in Pearson\n",
    "        i2 - int, for each test, the index of number to form second list in Pearson\n",
    "                e.g. i1==2; i2==4 means we select second and forth numbers in the tests results to do Pearson test\n",
    "    Returns:\n",
    "        (float, float) - a tuple of (coeficient, p-value)\n",
    "    '''\n",
    "    \n",
    "    a = []\n",
    "    b = []\n",
    "    for t in tuples:\n",
    "        a.append(t[i1])\n",
    "        b.append(t[i2])\n",
    "    assert(len(a)==len(b))    \n",
    "    return pearson(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43dff0c1-83e5-4e0b-b540-10e737928e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSets = ['EN-RG-65.txt', 'EN-WS-353-ALL.txt', 'EN-RW-STANFORD.txt', 'EN-MEN-TR-3k.txt', 'EN-MTurk-287.txt', 'EN-MTurk-771.txt', 'EN-SIMLEX-999.txt', 'EN-SimVerb-3500.txt']\n",
    "\n",
    "\n",
    "def similarity_eval(dataSetAddress, wordVecModel_str):\n",
    "    \n",
    "    wordVecModel = eval(wordVecModel_str)\n",
    "    vocab = set(list(wordVecModel.keys()))\n",
    "    \n",
    "    fread_simlex = open(dataSetAddress, \"r\")\n",
    "    \n",
    "    pair_list = []\n",
    "\n",
    "    line_number = 0\n",
    "    for line in fread_simlex:\n",
    "\n",
    "        tokens = line.split()\n",
    "        word_i = tokens[0]\n",
    "        word_j = tokens[1]\n",
    "        score = float(tokens[2])\n",
    "        if word_i in vocab and word_j in vocab:\n",
    "            pair_list.append( ((word_i, word_j), score) )\n",
    "\n",
    "    pair_list.sort(key=lambda x: - x[1]) # order the pairs from highest score (most similar) to lowest score (least similar)\n",
    "\n",
    "\n",
    "    extracted_scores = {}\n",
    "\n",
    "    extracted_list = []\n",
    "    \n",
    "               \n",
    "    for (x,y) in pair_list:\n",
    "        (word_i, word_j) = x\n",
    "        \n",
    "        current_distance = 1- cosine_similarity( wordVecModel[word_i].reshape(1,-1)  , wordVecModel[word_j].reshape(1,-1) )        \n",
    "\n",
    "        extracted_scores[(word_i, word_j)] = current_distance\n",
    "        extracted_list.append(((word_i, word_j), current_distance))\n",
    "\n",
    "    extracted_list.sort(key=lambda x: x[1])\n",
    "\n",
    "    spearman_original_list = []\n",
    "    spearman_target_list = []\n",
    "\n",
    "    for position_1, (word_pair, score_1) in enumerate(pair_list):\n",
    "        score_2 = extracted_scores[word_pair]\n",
    "        position_2 = extracted_list.index((word_pair, score_2))\n",
    "        spearman_original_list.append(position_1)\n",
    "        spearman_target_list.append(position_2)\n",
    "\n",
    "    spearman_rho = spearmanr(spearman_original_list, spearman_target_list)\n",
    "    \n",
    "    return spearman_rho[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6d7400c-b345-4e74-9f32-4851628d9ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Similarity task\n",
      "Good result is higher\n",
      "\n",
      "EN-RG-65.txt\n",
      "0.7794\n",
      "EN-WS-353-ALL.txt\n",
      "0.6856\n",
      "EN-RW-STANFORD.txt\n",
      "0.3970\n",
      "EN-MEN-TR-3k.txt\n",
      "0.7484\n",
      "EN-MTurk-287.txt\n",
      "0.6452\n",
      "EN-MTurk-771.txt\n",
      "0.6741\n",
      "EN-SIMLEX-999.txt\n",
      "0.3765\n",
      "EN-SimVerb-3500.txt\n",
      "0.2286\n"
     ]
    }
   ],
   "source": [
    "print('Word Similarity task\\nGood result is higher\\n')\n",
    "resourceFile = './Data/' \n",
    "\n",
    "for dataset in dataSets:\n",
    "    dataSetAddress = resourceFile + 'WordSim/' +  dataset\n",
    "    print(dataset)\n",
    "    print('%.4f' %  similarity_eval(dataSetAddress, 'debias_word'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6efd077-e2ca-46cb-9524-9284151a6683",
   "metadata": {},
   "source": [
    "# GBWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50ae5057-aec4-4d2b-b9ef-a078ec2a8231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Normalize the word embedding\n",
    "'''\n",
    "def normalize(word_embedding):\n",
    "    for word in word_embedding.keys():\n",
    "        vec = word_embedding[word]\n",
    "        norm = np.linalg.norm(vec)\n",
    "        word_embedding[word] = vec / norm\n",
    "\n",
    "def check_normalize(word_embedding):\n",
    "    count = 0\n",
    "    for word in word_embedding.keys():\n",
    "        vec = word_embedding[word]\n",
    "        if np.linalg.norm(vec) > 1.0001 or np.linalg.norm(vec) < 0.9999:\n",
    "            print(word)\n",
    "            print(count)\n",
    "            raise Exception(\"Normalize error\")\n",
    "            \n",
    "def dim299(word_embedding):\n",
    "    for w in word_embedding:\n",
    "        try:\n",
    "            word_embedding[w] = word_embedding[w][:-1]\n",
    "            assert(len(word_embedding[w]) == 299)\n",
    "        except Exception:\n",
    "            print(w)\n",
    "\n",
    "normalize(glove_word)\n",
    "check_normalize(glove_word)\n",
    "print(\"Done\")\n",
    "debias_copy = debias_word.copy()\n",
    "normalize(debias_word)\n",
    "check_normalize(debias_word)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5808914-c498-4095-b36f-83be7b73ff83",
   "metadata": {},
   "source": [
    "## WEAT Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5edbdb09-2355-40bf-b416-919e5b19107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.misc as misc\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def similarity(w1, w2):\n",
    "    \n",
    "    return w1.dot(w2)\n",
    "\n",
    "def s_word(w, A, B, word_embedding, all_s_words):\n",
    "    '''\n",
    "    Calculate association s(w, A, B) in WEAT test\n",
    "    Args:\n",
    "        w - str, the word for calculate\n",
    "        A - list[str], attribute set 1\n",
    "        B - list[str], attribute set 2\n",
    "        word_embedding - dict{str:np.array}, {word:vector} word vectors dictionary\n",
    "        all_s_words - dict{str:float}, {word:value of s(w, A, B)} record the result of the word\n",
    "    Returns:\n",
    "        float - value of s(w, A, B)\n",
    "    '''\n",
    "    if w in all_s_words:\n",
    "        return all_s_words[w]\n",
    "    \n",
    "    mean_a = []\n",
    "    mean_b = []\n",
    "    \n",
    "    for a in A:\n",
    "        mean_a.append(similarity(word_embedding[w], word_embedding[a]))\n",
    "    for b in B:\n",
    "        mean_b.append(similarity(word_embedding[w], word_embedding[b]))\n",
    "        \n",
    "    mean_a = sum(mean_a)/float(len(mean_a))\n",
    "    mean_b = sum(mean_b)/float(len(mean_b))\n",
    "    \n",
    "    all_s_words[w] = mean_a - mean_b\n",
    "\n",
    "    return all_s_words[w]\n",
    "\n",
    "\n",
    "def s_group(X, Y, A, B, word_embedding, all_s_words):\n",
    "    '''\n",
    "    Calculate test statistic s(X, Y, A, B) in WEAT test\n",
    "    Args:\n",
    "        X - list[str], target set 1\n",
    "        Y - list[str], target set 2\n",
    "        A - list[str], attribute set 1\n",
    "        B - list[str], attribute set 2\n",
    "        word_embedding - dict{str:np.array}, {word:vector} word vectors dictionary\n",
    "        all_s_words - dict{str:float}, {word:value of s(w, A, B)} record the result of each word\n",
    "    Returns:\n",
    "        float - value of s(X, Y, A, B)\n",
    "    '''\n",
    "    total = 0\n",
    "    for x in X:\n",
    "        total += s_word(x, A, B, word_embedding, all_s_words)\n",
    "    for y in Y:\n",
    "        total -= s_word(y, A, B, word_embedding, all_s_words)\n",
    "        \n",
    "    return total\n",
    "\n",
    "def effect_size(X, Y, A, B, word_embedding, all_s_words):\n",
    "    '''\n",
    "    Calculate effect size d in WEAT test\n",
    "    Args:\n",
    "        X - list[str], target set 1\n",
    "        Y - list[str], target set 2\n",
    "        A - list[str], attribute set 1\n",
    "        B - list[str], attribute set 2\n",
    "        word_embedding - dict{str:np.array}, {word:vector} word vectors dictionary\n",
    "        all_s_words - dict{str:float}, {word:value of s(w, A, B)} record the result of each word\n",
    "    Returns:\n",
    "        float - effect size d\n",
    "    '''\n",
    "    total_x = []\n",
    "    total_y = []\n",
    "    \n",
    "    for x in X:\n",
    "        total_x.append(s_word(x, A, B, word_embedding, all_s_words))\n",
    "    for y in Y:\n",
    "        total_y.append(s_word(y, A, B, word_embedding, all_s_words))\n",
    "        \n",
    "    \n",
    "    mean = sum(total_x)/float(len(total_x)) - sum(total_y)/float(len(total_y))\n",
    "    std_dev = np.std(total_x + total_y)\n",
    "    \n",
    "    if std_dev == 0:\n",
    "        print(\"Error! \")\n",
    "    \n",
    "    return mean / std_dev\n",
    "\n",
    "\n",
    "def p_value_exhust(X, Y, A, B, word_embedding):\n",
    "    '''\n",
    "    WEAT test\n",
    "    Args:\n",
    "        X - list[str], target set 1\n",
    "        Y - list[str], target set 2\n",
    "        A - list[str], attribute set 1\n",
    "        B - list[str], attribute set 2\n",
    "        word_embedding - dict{str:np.array}, {word:vector} word vectors dictionary\n",
    "    Returns:\n",
    "        tuple(float, float) - (test statistic, effect size)\n",
    "    '''\n",
    "    \n",
    "\n",
    "    if len(X) > 10:\n",
    "        print('might take too long, use sampled version: p_value')\n",
    "        return\n",
    "    \n",
    "    assert(len(X) == len(Y))\n",
    "    \n",
    "    all_s_words = {}\n",
    "    s_orig = s_group(X, Y, A, B, word_embedding, all_s_words) \n",
    "    \n",
    "    union = set(X+Y)\n",
    "    subset_size = int(len(union)/2)\n",
    "    \n",
    "    larger = 0\n",
    "    total = 0\n",
    "\n",
    "    for subset in set(itertools.combinations(union, subset_size)):\n",
    "        total += 1\n",
    "        Xi = list(set(subset))\n",
    "        Yi = list(union - set(subset))\n",
    "        if s_group(Xi, Yi, A, B, word_embedding, all_s_words) > s_orig:\n",
    "            larger += 1\n",
    "    \n",
    "    d = effect_size(X, Y, A, B, word_embedding, all_s_words)\n",
    "    \n",
    "    return larger/float(total), d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97e671a0-ad6d-474b-9801-8cae54f57bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weat_test(word_embedding):\n",
    "    final = \"Experiment 1 : \"\n",
    "\n",
    "    A = ['john', 'paul', 'mike', 'kevin', 'steve', 'greg', 'jeff', 'bill']\n",
    "    B = ['amy', 'joan', 'lisa', 'sarah', 'diana', 'kate', 'ann', 'donna']\n",
    "    C = ['caress', 'freedom', 'health', 'love', 'peace', 'cheer', 'friend', 'heaven', 'loyal', 'pleasure', 'diamond', 'gentle', 'honest', 'lucky', 'rainbow', 'diploma', 'gift', 'honor', 'miracle', 'sunrise', 'family', 'happy', 'laughter', 'paradise', 'vacation']\n",
    "    D = ['abuse', 'crash', 'filth', 'murder', 'sickness', 'accident', 'death', 'grief', 'poison', 'stink', 'assault', 'disaster', 'hatred', 'pollute', 'tragedy', 'divorce', 'jail', 'poverty', 'ugly', 'cancer', 'kill', 'rotten', 'vomit', 'agony', 'prison']\n",
    "    \n",
    "    result = p_value_exhust(A, B, C, D, word_embedding)\n",
    "    final += \"p-value: \" + str(result[0]) + \"\\t effect size: \" + str(result[1]) + '\\n'\n",
    "\n",
    "    final += \"Experiment 2 : \"\n",
    "\n",
    "    E = ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career']\n",
    "    F = ['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives']\n",
    "\n",
    "    result = p_value_exhust(A, B, E, F, word_embedding)\n",
    "    final += \"p-value: \" + str(result[0]) + \"\\t effect size: \" + str(result[1]) + '\\n'\n",
    "\n",
    "\n",
    "    final +=  \"Experiment 3 : \"\n",
    "\n",
    "    G = ['science', 'technology', 'physics', 'chemistry', 'einstein', 'nasa', 'experiment', 'astronomy']\n",
    "    H = ['poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture']\n",
    "\n",
    "    \n",
    "    result = p_value_exhust(A, B, G, H, word_embedding)\n",
    "    final += \"p-value: \" + str(result[0]) + \"\\t effect size: \" + str(result[1]) + '\\n'\n",
    "    \n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c54baade-78ba-4aa5-9230-39dd026d1390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1 : p-value: 0.755011655011655\t effect size: -0.372513251970572\n",
      "Experiment 2 : p-value: 0.0005439005439005439\t effect size: 1.4591173594726985\n",
      "Experiment 3 : p-value: 0.4864024864024864\t effect size: 0.01903524369699523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(weat_test(debias_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8497d595-fc58-4bb0-81d6-f16e013371e6",
   "metadata": {},
   "source": [
    "## Gender-projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efa9147d-dfe6-4d43-96fd-4c73b91e3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of the bias, before and after\n",
    "\n",
    "def compute_bias_by_projection(word_embedding, word_list):\n",
    "    '''\n",
    "    Calculate gender projection on gender direction for each word\n",
    "    Args:\n",
    "        word_embedding - dict{str:np.array}, {word:vector} word vectors dictionary\n",
    "        word_list - list[str], the evaluation word list\n",
    "    Returns:\n",
    "        dict{str:float} - {word:projection}, record the projection of each word in the evaluation word list\n",
    "    '''\n",
    "    direction = word_embedding['he'] - word_embedding['she']\n",
    "    d = {}\n",
    "    for w in word_list:\n",
    "        try:\n",
    "            d[w] = word_embedding[w].dot(direction)\n",
    "        except Exception:\n",
    "            print(w)\n",
    "            continue\n",
    "    return d\n",
    "\n",
    "\n",
    "word_project = compute_bias_by_projection(glove_word, word_limited)\n",
    "debias_project = compute_bias_by_projection(debias_word, word_limited)\n",
    "\n",
    "# calculate the avg bias of the vocabulary (abs) before and after debiasing\n",
    "\n",
    "def report_bias(gender_bias):\n",
    "    '''\n",
    "    Calculate average word projection in the evaluation word list\n",
    "    Args:\n",
    "        gender_bias - dict{str:float}, {word:projection} dictionary records projection of word\n",
    "    Returns:\n",
    "        float - average gender projection\n",
    "    '''\n",
    "    bias = 0.0\n",
    "    for k in gender_bias:\n",
    "        bias += np.abs(gender_bias[k])\n",
    "    return bias/len(gender_bias)\n",
    "\n",
    "dim299(debias_copy)\n",
    "normalize(debias_copy)\n",
    "check_normalize(debias_copy)\n",
    "del debias_word\n",
    "debias_word = debias_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90da48a0-a0cd-4ed9-be18-4684652edb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debias word : 0.0037179926018053107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"debias word : {}\\n\".format(report_bias(debias_project)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c533a9-d34a-4bf2-a4dc-84a1b0f36dcc",
   "metadata": {},
   "source": [
    "## Correlation between bias-by-projection and bias-by-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38570e4d-7d6f-40cf-b157-8d1afb1cfe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vec(word_embedding, word_list):\n",
    "    '''\n",
    "    Integrete the word vectors in the evaluation word list in a matrix.\n",
    "    Args:\n",
    "        word_embedding - dict{str:np.array}, {word:vector} word vectors dictionary\n",
    "        word_list - list[str], the evaluation word list\n",
    "    Returns:\n",
    "        np.array(number of words, number of dimentions) - each row is a word vector, the order is same as the word list\n",
    "    '''\n",
    "    word_limited_vec = []\n",
    "    for w in word_list:\n",
    "        word_limited_vec.append(word_embedding[w])\n",
    "    word_limited_vec = np.array(word_limited_vec)\n",
    "    return word_limited_vec\n",
    "\n",
    "word_limited_vec = word_vec(glove_word, word_limited)\n",
    "debias_limited_vec = word_vec(debias_word, word_limited)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "967c7e18-3725-46f7-910e-f0af4fd096e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 47698/47698 [03:56<00:00, 201.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# get tuples of biases and counts of masculine/feminine NN for each word (for bias-by-neighbors)\n",
    "\n",
    "def bias_by_neighbors(word_embedding, word_list, word_limited_vec, gender_bias_project, neighbours_num = 105):\n",
    "    '''\n",
    "    Calculate the gender-biased words in the k nearest neighbors of each word in the word list\n",
    "    Args:\n",
    "        word_embedding - dict{str:np.array}, {word:vector} word vectors dictionary\n",
    "        word_list - list[str], the evaluation word list\n",
    "        word_limited_vec - np.array(number of words, number of dimentions),\n",
    "                            each row is a word vector, the order is same as the word list\n",
    "        gender_bias_project - dict{str:float}, {word:projection} record the projection of each word in the evaluation word list\n",
    "        neighbours_num - int, select top k similar neighbors\n",
    "    Returns:\n",
    "        tuple(str, np.array, int) - (word, word vector, the number of male words in top k similar neighbors)\n",
    "    '''\n",
    "    tuples = []\n",
    "    count = 0\n",
    "    for w in tqdm(word_list):\n",
    "        \n",
    "        vec = word_embedding[w]\n",
    "        sim = word_limited_vec.dot(vec)\n",
    "        sort_sim = (sim.argsort())[::-1]\n",
    "        best = sort_sim[:(neighbours_num+1)]\n",
    "        \n",
    "        top = []\n",
    "        for i in best:\n",
    "            if i != count:\n",
    "                top.append(word_list[i])\n",
    "            elif word_list[i] != word_list[count]:\n",
    "                raise Exception(\"Same Word Error\")\n",
    "        \n",
    "\n",
    "        m = 0\n",
    "        f = 0    \n",
    "        for t in top[:100]:\n",
    "            if gender_bias_project[t] > 0:\n",
    "                m+=1\n",
    "            else:\n",
    "                f+=1\n",
    "            \n",
    "        tuples.append((w, word_project[w], m))\n",
    "        count = count + 1\n",
    "\n",
    "    return tuples\n",
    "\n",
    "tuples_debias = bias_by_neighbors(debias_word, word_limited, debias_limited_vec, word_project) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "353e4bed-1957-4a01-b08b-d3b63e9bce93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBWR correlation task :\n",
      "k-nearest neighbor of 50000 words, percent of male in k = 100\n",
      "Good result is lower\n",
      "debias word : \n",
      "(0.6431230166273049, 0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GBWR correlation task :\\nk-nearest neighbor of 50000 words, percent of male in k = 100\\nGood result is lower\")\n",
    "\n",
    "print(\"debias word : \\n{}\\n\".format(compute_corr(tuples_debias, 1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921f183-2341-4e28-8b07-fc91aebcd2cb",
   "metadata": {},
   "source": [
    "## Clustering Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf24dd53-a0f4-46bc-9fdb-9c23a7b0b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary finctions\n",
    "\n",
    "\n",
    "from cycler import cycler\n",
    "import operator\n",
    "import random\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "    \n",
    "def extract_vectors(words, before, after):\n",
    "    '''\n",
    "    Generate the word vector set\n",
    "    '''\n",
    "    size = len(words)/2\n",
    "    \n",
    "    X_bef = [before[x] for x in words]\n",
    "    X_aft = [after[x] for x in words]\n",
    "\n",
    "    return X_bef, X_aft\n",
    "\n",
    "    \n",
    "\n",
    "def cluster(words, X_bef, X_aft, random_state, y_true, num=2):\n",
    "    '''\n",
    "    Use K-Means to cluster the word vector set and calcualte the accuracy\n",
    "    Args:\n",
    "        words - list[str], the word list for clustering\n",
    "        X_bef - list[np.array], the list of the original word vectors\n",
    "        X_aft - list[np.array], the list of the debiased word vectors\n",
    "        random_state - int\n",
    "        y_true - list[int], the label (female/male) of the words\n",
    "        num - int, the number of clusters\n",
    "    Return:\n",
    "        str - the accuracy of clustering test\n",
    "    '''\n",
    "    final = ''\n",
    "    # fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n",
    "    \n",
    "    y_pred_bef = KMeans(n_clusters=num, random_state=random_state).fit_predict(X_bef)\n",
    "    # visualize(X_bef, words, y_pred_bef, axs[0], 'Original', random_state)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_bef) ]\n",
    "    final += \"original glove: {} \".format(sum(correct)/float(len(correct)))\n",
    "    \n",
    "    y_pred_aft = KMeans(n_clusters=num, random_state=random_state).fit_predict(X_aft)\n",
    "    for i in range(0, len(y_pred_aft)):\n",
    "        if y_pred_aft[i] == 0:\n",
    "            y_pred_aft[i] = 1\n",
    "        else:\n",
    "            y_pred_aft[i] = 0\n",
    "        \n",
    "    # visualize(X_aft, words, y_pred_aft, axs[1], 'Debiased', random_state)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_aft) ]\n",
    "    accuracy = sum(correct)/float(len(correct))\n",
    "    # print(accuracy)\n",
    "    if accuracy < 0.5:\n",
    "        accuracy = 1 - accuracy\n",
    "    final += \"compare with {} \".format(accuracy)\n",
    "    # fig.show()\n",
    "    # fig.savefig('HSR_clustering', bbox_inches='tight')\n",
    "    return final, accuracy\n",
    "\n",
    "\n",
    "# Cluster most biased words before and after debiasing\n",
    "def test(before, after):\n",
    "    '''\n",
    "    Generate word vector set and label set\n",
    "    Args:\n",
    "        before - dict{word:np.array}, the original word embedding\n",
    "        after - dict{word:np.array}, the debiased word embedding\n",
    "    Returns:\n",
    "        float - the accuracy of clustering test\n",
    "    '''\n",
    "    X_bef, X_aft = extract_vectors(male + female, before, after)\n",
    "    y_true = [0]*size + [1]*size\n",
    "    return cluster(male + female, X_bef, X_aft, random_state, y_true)\n",
    "    \n",
    "random.seed(1)\n",
    "random_state = 1\n",
    "\n",
    "size = 500\n",
    "sorted_g = sorted(word_project.items(), key=operator.itemgetter(1))\n",
    "female = [item[0] for item in sorted_g[:size]]\n",
    "male = [item[0] for item in sorted_g[-size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6caea991-515d-4cdb-97ca-9f6d1a30e431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBWR clustering task : \n",
      "clustering top 500, show which word is clustered wrongly\n",
      "Good result is lower\n",
      "original glove: 1.0 compare with 0.791 debias word\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GBWR clustering task : \")\n",
    "print(\"clustering top 500, show which word is clustered wrongly\")\n",
    "print(\"Good result is lower\")\n",
    "\n",
    "print(\"{}debias word\\n\".format(test(glove_word, debias_word)[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93701ad4-5f80-46c5-8db5-7f67ed850448",
   "metadata": {},
   "source": [
    "## Professional test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32e082c7-4e06-4096-8b43-724cb8e4a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def extract_professions():\n",
    "    '''\n",
    "    Read the professions list\n",
    "    Returns:\n",
    "        list[str] - the list of professions\n",
    "    '''\n",
    "    professions = []\n",
    "    with codecs.open('./Data/professions.json', 'r', 'utf-8') as f:\n",
    "        professions_data = json.load(f)\n",
    "    for item in professions_data:\n",
    "        professions.append(item[0].strip())\n",
    "    return professions\n",
    "\n",
    "\n",
    "professions = extract_professions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "305fa1cf-f7b5-4628-b32e-7cdadf1f3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuples_prof(word_embedding, words, word_list, word_limited_vec, gender_bias_dict):\n",
    "    '''\n",
    "    Calculate the gender-biased words in the k nearest neighbors for each profession\n",
    "    Args:\n",
    "        word_embedding - dict{str:np.array}, {word:vector} word vectors dictionary\n",
    "        words - list[str], the profession list\n",
    "        word_list - list[str], the evaluation word list\n",
    "        word_limited_vec - np.array(number of words, number of dimentions),\n",
    "                            each row is a word vector, the order is same as the word list\n",
    "        gender_bias_dict - dict{str:float}, {word:projection} record the projection of each word in the evaluation word list\n",
    "    Returns:\n",
    "        tuple(str, np.array, int) - (word, word vector, the number of male words in top k similar neighbors)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    tuples = []\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in gender_bias_dict:\n",
    "            continue\n",
    "            \n",
    "        # top = topK(w, space, k=105)[:100]\n",
    "        vec = word_embedding[w]\n",
    "        sim = word_limited_vec.dot(vec)\n",
    "        sort_sim = (sim.argsort())[::-1]\n",
    "        best = sort_sim[:(105+1)]\n",
    "        \n",
    "        # if w in words:\n",
    "        #     print(w)\n",
    "        # idx = word_list.index(w)\n",
    "        top = []\n",
    "        for i in best:\n",
    "            if word_list[i] != w:\n",
    "                top.append(word_list[i])\n",
    "            # else:\n",
    "                # print(\"___________________\")\n",
    "                # print(w)\n",
    "                # raise Exception(\"Same Word Error\")\n",
    "            \n",
    "        m = 0\n",
    "        f = 0  \n",
    "        for t in top[:100]:          \n",
    "            if gender_bias_dict[t] > 0:\n",
    "                m+=1\n",
    "            else:\n",
    "                f+=1\n",
    "                \n",
    "        tuples.append((w, gender_bias_dict[w], m))\n",
    "        \n",
    "        \n",
    "    return tuples\n",
    "\n",
    "prof_glove = get_tuples_prof(glove_word, professions, word_limited, word_limited_vec, word_project)\n",
    "prof_debias = get_tuples_prof(debias_word, professions, word_limited, debias_limited_vec, word_project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c75fa634-7c11-4601-b7e2-a035c30cdb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBWR professional task : \n",
      "k-nearest neighbor of professional words, percent of male in k = 100\n",
      "Good result is lower\n",
      "\n",
      "debias word : \n",
      "0.7096458133861437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GBWR professional task : \\nk-nearest neighbor of professional words, percent of male in k = 100\\nGood result is lower\\n\")\n",
    "print(\"debias word : \\n{}\\n\".format(compute_corr(prof_debias, 1, 2)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66521f3-c185-4c11-9f5a-35308fc86fa8",
   "metadata": {},
   "source": [
    "## Classification test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "231a20e0-57e1-497a-bed6-accea8a29af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "\n",
    "\n",
    "def train_and_predict(word_embedding):\n",
    "    '''\n",
    "    Train an SVM classifier and calculate the accuracy of prediction\n",
    "    Args:\n",
    "        word_embedding - dict{str:np.array}, {word:vector} word vectors dictionary\n",
    "    Returns:\n",
    "        float - the accuracy of prediction\n",
    "    '''\n",
    "    X_train = [word_embedding[w] for w in males[:size_train]+females[:size_train]]\n",
    "    Y_train = [1]*size_train + [0]*size_train\n",
    "    X_test = [word_embedding[w] for w in males[size_train:]+females[size_train:]]\n",
    "    Y_test = [1]*size_test + [0]*size_test\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    preds = clf.predict(X_test)\n",
    "\n",
    "    accuracy = [1 if y==z else 0 for y,z in zip(preds, Y_test)]\n",
    "    return float(sum(accuracy))/len(accuracy)\n",
    "\n",
    "    \n",
    "# extract most biased words\n",
    "size_train = 500\n",
    "size_test = 2000\n",
    "size = size_train + size_test\n",
    "sorted_g = sorted(word_project.items(), key=operator.itemgetter(1))\n",
    "females = [item[0] for item in sorted_g[:size]]\n",
    "males = [item[0] for item in sorted_g[-size:]]\n",
    "for f in females:\n",
    "    assert(word_project[f] < 0)\n",
    "for m in males:\n",
    "    assert(word_project[m] > 0)\n",
    "shuffle(females)\n",
    "shuffle(males)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "951f1b31-f917-453c-97ce-367be806e2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBWR classification task : \n",
      "top 25000 female and male, SVM is trained by 10000 words and predict other 40000\n",
      "Good result is lower\n",
      "\n",
      "debias word : 0.85475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GBWR classification task : \\ntop 25000 female and male, SVM is trained by 10000 words and predict other 40000\\nGood result is lower\\n\")\n",
    "\n",
    "print('debias word : {}\\n'.format(train_and_predict(debias_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257cf70-fe2f-4d11-836c-801086b687ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
