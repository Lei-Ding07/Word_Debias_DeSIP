{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df72caf-0ccb-4172-acc2-f7805632f0be",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "Manzini, T.; Lim, Y. C.; Tsvetkov, Y.; and Black, A. W.2019. Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings. NAACL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d48a630-6dc4-4199-9ecf-892cecb959e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import random\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from vocab import Vocab\n",
    "from featurize import formatBatchedExamples, constructEmbeddingTensorFromVocabAndWvs, getExampleSubset\n",
    "from models.POSTagger import POSTagger\n",
    "from modelUtil import train_step, test, precisionRecallEval\n",
    "\n",
    "from DataLoader import loadNERDatasetXY\n",
    "from DataBatch import SeqDataBatch\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd464f4-2c6e-4042-bfd0-497c72441fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data parameters (The whole pipeline will need to rerun if these are changed)\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "TRAIN_PERCENT = 0.8\n",
    "VAL_PERCENT = 0.1\n",
    "TEST_PERCENT = 0.1\n",
    "\n",
    "DATA_TARGET_POS = 1\n",
    "DATA_TARGET_CHUNK = 2\n",
    "\n",
    "#Learning Parameters (Only the model training code will need to rerun if these are changed)\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 25\n",
    "USE_CUDA = False\n",
    "DEBUG_INTERVAL = 250\n",
    "L2_REG = 0.001\n",
    "MOMENTUM = 0.25\n",
    "\n",
    "DEBIAS_EPS = 1e-3\n",
    "EMBEDDING_SIZE = 300\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8204cf75-182c-4537-bcc2-2bafedaf8f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322636\n"
     ]
    }
   ],
   "source": [
    "VEC_LEN = 300\n",
    "glove_file = open(\"../Data/WordEmbedding/glove_wiki_vectors.txt\", 'r')\n",
    "glove_word = {}\n",
    "for line in glove_file:\n",
    "    line = line.strip()\n",
    "    _word = line.split(' ')\n",
    "    vector = np.array([float(num) for num in _word[1:]])\n",
    "    if len(vector) != VEC_LEN: \n",
    "        raise Exception(\"Word dimension is wrong\")\n",
    "    glove_word[_word[0]] = vector\n",
    "glove_file.close()\n",
    "print(len(glove_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb08f30-3643-44ec-9778-1198bcadd6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debias word vector file name\n",
    "file_name = \"P_DeSIP_vectors.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0887581a-2e46-4ab4-b5e7-44878cd81a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "VEC_LEN = 300\n",
    "debias_file = open(\"../Data/WordEmbedding/\" + file_name, 'r')\n",
    "debias_word = {}\n",
    "for line in debias_file:\n",
    "    line = line.strip()\n",
    "    _word = line.split(' ')\n",
    "    vector = np.array([float(num) for num in _word[1:]])\n",
    "    if len(vector) != VEC_LEN: \n",
    "        raise Exception(\"Word dimension is wrong\")\n",
    "    debias_word[_word[0]] = vector\n",
    "debias_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a738f8-235a-4594-acbf-a8488a290417",
   "metadata": {},
   "source": [
    "## torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843168d1-ed67-4f76-8c08-899b6fd4e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e13e81-a38c-452e-b52a-9bf2e38cc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSet(biasedWvs, debiasedWvs, task, batch_size = 5):\n",
    "    tokenize = lambda x: x.split()\n",
    "    X = Field(sequential=True,use_vocab=True,tokenize=tokenize,preprocessing=Pipeline(lambda x: x.lower()))\n",
    "    Y = Field(sequential=True,use_vocab=True,tokenize=tokenize,is_target=True,preprocessing=Pipeline(lambda x: x.lower()))\n",
    "    fields = {\n",
    "     'x':('X', X), \n",
    "     'y':('Y', Y)\n",
    "         }\n",
    "    data = TabularDataset.splits(path='./data/'+ task + '/', train='train.csv',validation='valid.csv',test='test.csv', format='csv', fields=fields)\n",
    "    X.build_vocab(data[0],specials=['<pad>'])\n",
    "    Y.build_vocab(data[0],specials=['<pad>'])\n",
    "    data_iter = BucketIterator.splits(data,batch_size=batch_size,device='cpu',shuffle=True,sort=False)\n",
    "    train_iter = data_iter[0]\n",
    "    val_iter = data_iter[1]\n",
    "    test_iter = data_iter[2]\n",
    "    \n",
    "    in_embed = []\n",
    "    wvTensor = [[0.0]*EMBEDDING_SIZE for _ in range(len(X.vocab.itos))]\n",
    "    for i in range(len(X.vocab.itos)):\n",
    "        try:\n",
    "            wvTensor[i] = biasedWvs[X.vocab.itos[i]]\n",
    "        except:\n",
    "            in_embed.append(i)\n",
    "            wvTensor[i] = [float(v) for v in np.random.rand(EMBEDDING_SIZE)]\n",
    "            #pdb.set_trace()\n",
    "    embedding_debias = wvTensor.copy()\n",
    "    embedding_org = torch.tensor(wvTensor)\n",
    "    print(\"length of vocab of org: \", len(embedding_org))\n",
    "    print(\"length of unknown: \", len(in_embed))\n",
    "    for i in range(len(X.vocab.itos)):\n",
    "        if i not in in_embed:\n",
    "            embedding_debias[i] = debiasedWvs[X.vocab.itos[i]]\n",
    "    embedding_debias = torch.tensor(embedding_debias)\n",
    "    print(\"length of vocab of debias: \", len(embedding_debias))\n",
    "    \n",
    "    return embedding_org, embedding_debias, X, Y, train_iter, val_iter, test_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3c19c-53bb-4c52-a2cc-0b6dfe65d223",
   "metadata": {},
   "source": [
    "## Embedding Matrix Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc0ab3d-713a-4264-9c94-67ae07544129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBiased(task, embedding_org, embedding_debias, X, Y, train_iter, val_iter, test_iter):\n",
    "    posModel = POSTagger(EMBEDDING_SIZE, 20, len(X.vocab.itos), len(Y.vocab.itos))\n",
    "    posModel.setEmbeddings(embedding_org, freeze=True)\n",
    "    device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "    optimizer = optim.RMSprop(posModel.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=L2_REG)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(\"Starting Training\")\n",
    "\n",
    "    start = True\n",
    "    bestValLoss = 1000 #test(posModel, device, formattedBatchedValData, criterion) \n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        #for batch in train_iter:\n",
    "        \n",
    "        loss = train_step(posModel, device, train_iter, optimizer, criterion, epoch, DEBUG_INTERVAL)\n",
    "        #pdb.set_trace()\n",
    "        train_loss_record.append(loss)\n",
    "        val_loss = test(posModel, device, val_iter , criterion)\n",
    "        train_loss = test(posModel, device, train_iter , criterion)\n",
    "        result_val = precisionRecallEval(posModel, device, val_iter)\n",
    "        result_train = precisionRecallEval(posModel, device, train_iter)\n",
    "        # print(result)\n",
    "        # precision, recall, f1 = precisionRecallEval(posModel, device, formattedBatchedValData)\n",
    "        print(\"Epoch #{} - \\n\\tVal Loss: {:.6f} \\n\\tVal Precision {:6f} \\n\\tVal Recall {:6f} \\n\\tVal Macro F1 {:6f}\".format(epoch, val_loss, result_val[0], result_val[1], result_val[2]))\n",
    "        print(\"Epoch #{} - \\n\\tTrain Loss: {:.6f} \\n\\tTrain Precision {:6f} \\n\\tTrain Recall {:6f} \\n\\tTrain Macro F1 {:6f}\".format(epoch, train_loss, result_train[0], result_train[1], result_train[2]))\n",
    "    \n",
    "        if(val_loss < bestValLoss and not start):\n",
    "            torch.save(posModel.state_dict(), \"models/savedModels/model_\" + task + \"_org.m\")\n",
    "            bestValLoss = val_loss\n",
    "        start = False\n",
    "\n",
    "    print(\"Found best case validation loss to be \" + str(bestValLoss) + \"\\n\\t... Loading saved model and testing\")\n",
    "    posModel.load_state_dict(torch.load(\"models/savedModels/model_\" + task + \"_org.m\"))\n",
    "    test_loss = test(posModel, device, test_iter, criterion)\n",
    "    result = precisionRecallEval(posModel, device, test_iter)\n",
    "    print(\"TEST DATA -\\n\\tTest Loss: {:.6f} \\n\\tTest Precision {:6f} \\n\\tTest Recall {:6f} \\n\\tTest Macro F1 {:6f}\".format(test_loss, result[0], result[1], result[2]))\n",
    "\n",
    "    result_name = []\n",
    "    result = []\n",
    "    posModel = POSTagger(EMBEDDING_SIZE, 20, len(X.vocab.itos), len(Y.vocab.itos))\n",
    "    posModel.load_state_dict(torch.load(\"models/savedModels/model_\" + task + \"_org.m\"))\n",
    "    posModel.setEmbeddings(embedding_org, freeze=True)\n",
    "\n",
    "    biased_precision, biased_recall, biased_f1, _ = precisionRecallEval(posModel, device, test_iter)\n",
    "    test_biased_loss = test(posModel, device, test_iter, criterion)\n",
    "    result_name.extend([\"Bias\", \"loss: \", \"precision: \", \"recall: \", \"f1: \"])\n",
    "    result.extend([\" \", test_biased_loss, biased_precision, biased_recall, biased_f1])\n",
    "    print(\"============= BIASED EMBEDDINGS TEST RESULTS =============\")\n",
    "    print(\"loss: \" + str(test_biased_loss))\n",
    "    print(\"precision: \" + str(biased_precision))\n",
    "    print(\"recall: \" + str(biased_recall))\n",
    "    print(\"f1: \" + str(biased_f1))\n",
    "    \n",
    "    posModel = POSTagger(EMBEDDING_SIZE, 20, len(X.vocab.itos), len(Y.vocab.itos))\n",
    "    posModel.load_state_dict(torch.load(\"models/savedModels/model_\" + task + \"_org.m\"))\n",
    "    posModel.setEmbeddings(embedding_debias, freeze=True)\n",
    "\n",
    "    debiased_precision, debiased_recall, debiased_f1, _ = precisionRecallEval(posModel, device, test_iter)\n",
    "    test_debiased_loss = test(posModel, device, test_iter, criterion)\n",
    "    result_name.extend([\"Debias\", \"loss: \", \"precision: \", \"recall: \", \"f1: \"])\n",
    "    result.extend([\" \", test_debiased_loss, debiased_precision, debiased_recall, debiased_f1])\n",
    "    print(\"============= BIASED EMBEDDINGS TEST RESULTS =============\")\n",
    "    print(\"loss: \" + str(test_debiased_loss))\n",
    "    print(\"precision: \" + str(debiased_precision))\n",
    "    print(\"recall: \" + str(debiased_recall))\n",
    "    print(\"f1: \" + str(debiased_f1))\n",
    "    \n",
    "    print(\"============= EMBEDDINGS COMPARISION RESULTS =============\")\n",
    "    print(\"delta loss: \" + str(test_debiased_loss - test_biased_loss))\n",
    "    print(\"delta precision: \" + str(debiased_precision - biased_precision))\n",
    "    print(\"delta recall: \" + str(debiased_recall - biased_recall))\n",
    "    print(\"delta f1: \" + str(debiased_f1 - biased_f1))\n",
    "\n",
    "    return result_name, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0300413-7c92-4397-ab14-fb25eead285e",
   "metadata": {},
   "source": [
    "## Model Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3b442bb-f9c0-4770-af05-d47155da3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDebiased(task, embedding_org, embedding_debias, X, Y, train_iter, val_iter, test_iter):\n",
    "    posModel = POSTagger(EMBEDDING_SIZE, 20, len(X.vocab.itos), len(Y.vocab.itos))\n",
    "    posModel.setEmbeddings(embedding_debias, freeze=True)\n",
    "    \n",
    "    optimizer = optim.RMSprop(posModel.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=L2_REG)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(\"Starting Training\")\n",
    "\n",
    "    start = True\n",
    "    bestValLoss = 1000 #test(posModel, device, formattedBatchedValData, criterion) \n",
    "    train_loss_record = []\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        #for batch in train_iter:\n",
    "\n",
    "        loss = train_step(posModel, device, train_iter, optimizer, criterion, epoch, DEBUG_INTERVAL)\n",
    "        #pdb.set_trace()\n",
    "        train_loss_record.append(loss)\n",
    "        val_loss = test(posModel, device, val_iter , criterion)\n",
    "        train_loss = test(posModel, device, train_iter , criterion)\n",
    "        result_val = precisionRecallEval(posModel, device, val_iter)\n",
    "        result_train = precisionRecallEval(posModel, device, train_iter)\n",
    "        # print(result)\n",
    "        # precision, recall, f1 = precisionRecallEval(posModel, device, formattedBatchedValData)\n",
    "        print(\"Epoch #{} - \\n\\tVal Loss: {:.6f} \\n\\tVal Precision {:6f} \\n\\tVal Recall {:6f} \\n\\tVal Macro F1 {:6f}\".format(epoch, val_loss, result_val[0], result_val[1], result_val[2]))\n",
    "        print(\"Epoch #{} - \\n\\tTrain Loss: {:.6f} \\n\\tTrain Precision {:6f} \\n\\tTrain Recall {:6f} \\n\\tTrain Macro F1 {:6f}\".format(epoch, train_loss, result_train[0], result_train[1], result_train[2]))\n",
    "\n",
    "        if(val_loss < bestValLoss and not start):\n",
    "            torch.save(posModel.state_dict(), \"models/savedModels/model_\" + task + \"_debias.m\")\n",
    "            bestValLoss = val_loss\n",
    "        start = False\n",
    "\n",
    "    print(\"Found best case validation loss to be \" + str(bestValLoss) + \"\\n\\t... Loading saved model and testing\")\n",
    "    posModel.load_state_dict(torch.load(\"models/savedModels/model_\" + task + \"_debias.m\"))\n",
    "    test_loss = test(posModel, device, test_iter, criterion)\n",
    "    result = precisionRecallEval(posModel, device, test_iter)\n",
    "    print(\"TEST DATA -\\n\\tTest Loss: {:.6f} \\n\\tTest Precision {:6f} \\n\\tTest Recall {:6f} \\n\\tTest Macro F1 {:6f}\".format(test_loss, result[0], result[1], result[2]))\n",
    "    \n",
    "    result = []\n",
    "    posModel = POSTagger(EMBEDDING_SIZE, 20, len(X.vocab.itos), len(Y.vocab.itos))\n",
    "    posModel.load_state_dict(torch.load(\"models/savedModels/model_\" + task + \"_debias.m\"))\n",
    "    posModel.setEmbeddings(embedding_debias, freeze=True)\n",
    "\n",
    "    debiased_precision, debiased_recall, debiased_f1, _ = precisionRecallEval(posModel, device, test_iter)\n",
    "    test_debiased_loss = test(posModel, device, test_iter, criterion)\n",
    "    \n",
    "    result.extend([\" \", test_debiased_loss, debiased_precision, debiased_recall, debiased_f1])\n",
    "    print(\"============= DEBIASED EMBEDDINGS TEST RESULTS =============\")\n",
    "    print(\"loss: \" + str(test_debiased_loss))\n",
    "    print(\"precision: \" + str(debiased_precision))\n",
    "    print(\"recall: \" + str(debiased_recall))\n",
    "    print(\"f1: \" + str(debiased_f1))\n",
    "\n",
    "    return result\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a9af708-4ce8-472f-90ec-ffb7c4c5d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(resultBias, resultDebias, nameBias):\n",
    "    result = []\n",
    "    result_name = []\n",
    "    result_name.extend(nameBias)\n",
    "    result.extend(resultBias)\n",
    "    \n",
    "    result_name.extend([\"Comparison\", \"delta loss: \", \"delta precision: \", \"delta recall: \", \"delta f1: \"])\n",
    "    result.extend([\" \", resultBias[6] - resultBias[1], resultBias[7] - resultBias[2], resultBias[8] - resultBias[3], resultBias[9] - resultBias[4]])\n",
    "    \n",
    "    \n",
    "    result_name.extend([\"Debias train\", \"loss: \", \"precision: \", \"recall: \", \"f1: \"])\n",
    "    result.extend(resultDebias)\n",
    "    \n",
    "    result_name.extend([\"Comparison\", \"delta loss: \", \"delta precision: \", \"delta recall: \", \"delta f1: \"])\n",
    "    result.extend([\" \", resultDebias[1] - resultBias[1], resultDebias[2] - resultBias[2], resultDebias[3] - resultBias[3], resultDebias[4] - resultBias[4]])\n",
    "\n",
    "    return result_name, result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4e21a06-7e55-44a6-8255-4115a30d825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(task, biasedWvs, debiasedWvs):\n",
    "    embedding_org, embedding_debias, X, Y, train_iter, val_iter, test_iter = getDataSet(biasedWvs, debiasedWvs, task, 5)\n",
    "    nameBias, resultBias = trainBiased(task, embedding_org, embedding_debias, X, Y, train_iter, val_iter, test_iter)\n",
    "\n",
    "    resultDebias = trainDebiased(task, embedding_org, embedding_debias, X, Y, train_iter, val_iter, test_iter)\n",
    "\n",
    "    name, number = calculate(resultBias, resultDebias, nameBias)\n",
    "    \n",
    "    return name, number\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "597b477b-5ad7-4af7-91a2-f0106fad0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_record = []\n",
    "final_name = []\n",
    "final_result = []\n",
    "for task in [\"pos\", \"ner\", \"chunking\"]:\n",
    "    final_name.append(task)\n",
    "    final_result.append(\" \")\n",
    "    result_name, result = main(task, glove_word, debias_word)\n",
    "    final_name.extend(result_name)\n",
    "    final_result.extend(result)\n",
    "df = pd.DataFrame({\"label\":final_name, file_name:final_result})\n",
    "df.to_csv(\"../Result/performance/\" + file_name.replace('/', '_') + \"1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3abbccbb-641e-4325-b4c3-c03ecc68cb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Hard_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bias</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loss:</td>\n",
       "      <td>0.372697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precision:</td>\n",
       "      <td>0.938881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recall:</td>\n",
       "      <td>0.874931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Comparison</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>delta loss:</td>\n",
       "      <td>0.076395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>delta precision:</td>\n",
       "      <td>0.007522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>delta recall:</td>\n",
       "      <td>-0.043787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>delta f1:</td>\n",
       "      <td>-0.010649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                label Hard_bias\n",
       "0                 pos          \n",
       "1                Bias          \n",
       "2              loss:   0.372697\n",
       "3         precision:   0.938881\n",
       "4            recall:   0.874931\n",
       "..                ...       ...\n",
       "73         Comparison          \n",
       "74       delta loss:   0.076395\n",
       "75  delta precision:   0.007522\n",
       "76     delta recall:  -0.043787\n",
       "77         delta f1:  -0.010649\n",
       "\n",
       "[78 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"label\":final_name, file_name:final_result})\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
